{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f71b9763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved matrix_f1f2_variant_smooth.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from typing import Optional, Dict\n",
    "\n",
    "from bmh.benchmark.material_deposition import MaterialDeposition, Material, Deposition\n",
    "from bmh.simulation.bsl_blending_simulator import BslBlendingSimulator\n",
    "from bmh.helpers.math import stdev\n",
    "from bmh.helpers.stockpile_math import get_stockpile_height, get_stockpile_slice_volume\n",
    "\n",
    "def random_walk(n, start=5, step_size=0.4):  # CHANGED: smaller step_size (smoother raw feed)\n",
    "    walk = [start]\n",
    "    for _ in range(n - 1):\n",
    "        step = np.random.uniform(-step_size, step_size)\n",
    "        next_value = max(0, walk[-1] + step)\n",
    "        walk.append(next_value)\n",
    "    return np.array(walk)\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    avg = np.average(values, weights=weights)\n",
    "    var = np.average((values - avg) ** 2, weights=weights)\n",
    "    return avg, math.sqrt(var)\n",
    "\n",
    "class ReclaimedMaterialEvaluator:\n",
    "    def __init__(self, reclaimed: Material, x_min: float, x_max: float):\n",
    "        self.reclaimed = reclaimed\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self._volume_stdev: Optional[float] = None\n",
    "\n",
    "    def get_volume_stdev(self) -> float:\n",
    "        if self._volume_stdev is None:\n",
    "            ideal_df = self.reclaimed.data.copy()\n",
    "            core_len = self.x_max - self.x_min\n",
    "            ideal_height = get_stockpile_height(volume=ideal_df['volume'].sum(), core_length=core_len)\n",
    "            ideal_df['x_diff'] = (ideal_df['x'] - ideal_df['x'].shift(1)).fillna(0.0)\n",
    "            ideal_df['volume'] = ideal_df.apply(\n",
    "                lambda row: get_stockpile_slice_volume(\n",
    "                    x=row['x'],\n",
    "                    core_length=core_len,\n",
    "                    height=ideal_height,\n",
    "                    x_min=self.x_min,\n",
    "                    x_diff=row['x_diff']\n",
    "                ),\n",
    "                axis=1\n",
    "            )\n",
    "            self._volume_stdev = stdev((ideal_df['volume'] - self.reclaimed.data['volume']).values)\n",
    "        return self._volume_stdev\n",
    "\n",
    "def generate_dataset_with_f1_f2():\n",
    "    BED_SIZE_X = 59\n",
    "    BED_SIZE_Z = 20\n",
    "\n",
    "    deposition_timestamps = np.linspace(0, 100, 20)\n",
    "    material_timestamps = np.linspace(0, 100, 50)\n",
    "    x_positions = np.random.uniform(BED_SIZE_Z * 0.5, BED_SIZE_X - BED_SIZE_Z * 0.5, 20)\n",
    "\n",
    "    # CHANGED: stronger smoothing -> smoother feed curve (tests different \"information curves\")\n",
    "    quality_values = random_walk(50, start=5, step_size=0.4)\n",
    "    quality_values = gaussian_filter(quality_values, sigma=2.0)  # CHANGED: sigma 2.0 (was ~1.0)\n",
    "    volume_values = np.ones(50) * 50\n",
    "\n",
    "    material = Material.from_data(pd.DataFrame({\n",
    "        'timestamp': material_timestamps,\n",
    "        'volume': volume_values,\n",
    "        'quality': quality_values\n",
    "    }))\n",
    "\n",
    "    deposition = Deposition.from_data(\n",
    "        data=pd.DataFrame({\n",
    "            'timestamp': deposition_timestamps,\n",
    "            'x': x_positions,\n",
    "            'z': [0.5 * BED_SIZE_Z] * len(x_positions)\n",
    "        }),\n",
    "        bed_size_x=BED_SIZE_X,\n",
    "        bed_size_z=BED_SIZE_Z,\n",
    "        reclaim_x_per_s=6  # unchanged on purpose\n",
    "    )\n",
    "\n",
    "    sim = BslBlendingSimulator(bed_size_x=BED_SIZE_X, bed_size_z=BED_SIZE_Z)\n",
    "    reclaimed = sim.stack_reclaim(MaterialDeposition(material=material, deposition=deposition))\n",
    "\n",
    "    y1 = weighted_avg_and_std(reclaimed.data['quality'], reclaimed.data['volume'])[1]\n",
    "    evaluator = ReclaimedMaterialEvaluator(reclaimed=reclaimed, x_min=float(np.min(x_positions)), x_max=float(np.max(x_positions)))\n",
    "    y2 = evaluator.get_volume_stdev()\n",
    "\n",
    "    row = {\n",
    "        'y1': y1, 'y2': y2,\n",
    "        **{f'x{i+1}': q for i, q in enumerate(material.data['quality'])},\n",
    "        **{f'x{i+51}': x for i, x in enumerate(deposition.data['x'])}\n",
    "    }\n",
    "    return pd.DataFrame([row])\n",
    "\n",
    "# ---- driver ----\n",
    "N_SAMPLES = 1000  # keep small for a test; scale up later if needed\n",
    "out = pd.concat([generate_dataset_with_f1_f2() for _ in range(N_SAMPLES)], ignore_index=True)\n",
    "out.to_csv('matrix_f1f2_variant_smooth.csv', index=False)\n",
    "print(\"Saved matrix_f1f2_variant_smooth.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0349db",
   "metadata": {},
   "source": [
    "Why these changes?\n",
    "\n",
    "Lower feed volatility (step_size=0.4 vs ~1.0) and stronger smoothing (sigma=2.0 vs ~1.0) model a more homogenized upstream feed. This directly tests: “How much do different material parameter information curves affect results?” by expecting lower y1 (quality stdev after reclaim). \n",
    "\n",
    "Keep bed geometry, deposition count, and reclaim speed the same so the only factor changing is the feed curve (clean ablation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63e1c7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix dataset saved to /Users/keithkumar/Desktop/Masters/Dissertation/ML evaluation/matrix_f1f2_200000.csv with 200000 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from typing import Optional, Dict\n",
    "\n",
    "from bmh.benchmark.material_deposition import MaterialDeposition, Material, Deposition\n",
    "from bmh.simulation.bsl_blending_simulator import BslBlendingSimulator\n",
    "from bmh.helpers.math import stdev\n",
    "from bmh.helpers.stockpile_math import get_stockpile_height, get_stockpile_slice_volume\n",
    "\n",
    "def random_walk(n, start=5, step_size=1.2):\n",
    "    walk = [start]\n",
    "    for _ in range(n - 1):\n",
    "        step = np.random.uniform(-step_size, step_size)\n",
    "        next_value = max(0, walk[-1] + step)\n",
    "        walk.append(next_value)\n",
    "    return np.array(walk)\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    avg = np.average(values, weights=weights)\n",
    "    var = np.average((values - avg) ** 2, weights=weights)\n",
    "    return avg, math.sqrt(var)\n",
    "\n",
    "class ReclaimedMaterialEvaluator:\n",
    "    def __init__(self, reclaimed: Material, x_min: float, x_max: float):\n",
    "        self.reclaimed = reclaimed\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self._volume_stdev: Optional[float] = None\n",
    "\n",
    "    def get_volume_stdev(self) -> float:\n",
    "        if self._volume_stdev is None:\n",
    "            ideal_df = self.reclaimed.data.copy()\n",
    "            core_len = self.x_max - self.x_min\n",
    "            ideal_height = get_stockpile_height(volume=ideal_df['volume'].sum(), core_length=core_len)\n",
    "            ideal_df['x_diff'] = (ideal_df['x'] - ideal_df['x'].shift(1)).fillna(0.0)\n",
    "            ideal_df['volume'] = ideal_df.apply(\n",
    "                lambda row: get_stockpile_slice_volume(\n",
    "                    x=row['x'],\n",
    "                    core_length=core_len,\n",
    "                    height=ideal_height,\n",
    "                    x_min=self.x_min,\n",
    "                    x_diff=row['x_diff']\n",
    "                ),\n",
    "                axis=1\n",
    "            )\n",
    "            self._volume_stdev = stdev((ideal_df['volume'] - self.reclaimed.data['volume']).values)\n",
    "        return self._volume_stdev\n",
    "\n",
    "def generate_dataset_with_f1_f2():\n",
    "    BED_SIZE_X = 70\n",
    "    BED_SIZE_Z = 20\n",
    "\n",
    "    deposition_timestamps = np.linspace(0, 100, 20)\n",
    "    material_timestamps = np.linspace(0, 100, 50)\n",
    "\n",
    "    x_positions = np.random.uniform(BED_SIZE_Z * 0.5, BED_SIZE_X - BED_SIZE_Z * 0.5, 20)\n",
    "\n",
    "    quality_values = random_walk(50, start=5, step_size=1.2)\n",
    "    quality_values = gaussian_filter(quality_values, sigma=0.5)\n",
    "    volume_values = np.ones(50) * 50\n",
    "\n",
    "    material = Material.from_data(pd.DataFrame({\n",
    "        'timestamp': material_timestamps,\n",
    "        'volume': volume_values,\n",
    "        'quality': quality_values\n",
    "    }))\n",
    "\n",
    "    deposition = Deposition.from_data(\n",
    "        data=pd.DataFrame({\n",
    "            'timestamp': deposition_timestamps,\n",
    "            'x': x_positions,\n",
    "            'z': [0.5 * BED_SIZE_Z] * len(x_positions)\n",
    "        }),\n",
    "        bed_size_x=BED_SIZE_X,\n",
    "        bed_size_z=BED_SIZE_Z,\n",
    "        reclaim_x_per_s=8\n",
    "    )\n",
    "\n",
    "    sim = BslBlendingSimulator(bed_size_x=BED_SIZE_X, bed_size_z=BED_SIZE_Z)\n",
    "    reclaimed = sim.stack_reclaim(MaterialDeposition(material=material, deposition=deposition))\n",
    "\n",
    "    y1 = weighted_avg_and_std(reclaimed.data['quality'], reclaimed.data['volume'])[1]\n",
    "    evaluator = ReclaimedMaterialEvaluator(\n",
    "        reclaimed=reclaimed,\n",
    "        x_min=float(np.min(x_positions)),\n",
    "        x_max=float(np.max(x_positions))\n",
    "    )\n",
    "    y2 = evaluator.get_volume_stdev()\n",
    "\n",
    "    row = {'y1': y1, 'y2': y2}\n",
    "    row.update({f'x{i+1}': q for i, q in enumerate(material.data['quality'])})\n",
    "    row.update({f'x{i+51}': x for i, x in enumerate(deposition.data['x'])})\n",
    "    return row  # return a dict (not a DataFrame)\n",
    "\n",
    "# ---- driver\n",
    "N = 200_000\n",
    "output_folder = '/Users/keithkumar/Desktop/Masters/Dissertation/ML evaluation'\n",
    "combined_file_path = f'{output_folder}/matrix_f1f2_200000.csv'\n",
    "\n",
    "rows = [generate_dataset_with_f1_f2() for _ in range(N)]\n",
    "combined_data = pd.DataFrame.from_records(rows)\n",
    "combined_data.to_csv(combined_file_path, index=False)\n",
    "print(f\"Matrix dataset saved to {combined_file_path} with {len(combined_data)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2e33ef",
   "metadata": {},
   "source": [
    "Longer bed (BED_SIZE_X=70) and faster reclaim (reclaim_x_per_s=8) probe operational parameter effects on y2 (volume stability), tying to your “optimization parameters / knee region” exploration. Faster reclaim typically amplifies volume fluctuations → higher y2 likely. \n",
    "\n",
    "Choppier raw feed (step_size=1.2, sigma=0.5) provides a contrasting “information curve” to Block 1, to see how the same feature layout responds when both upstream variability and operational parameters change. This should raise y1 and interact with the reclaim change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eff548",
   "metadata": {},
   "source": [
    "100k vs 200k (why 200k was chosen)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
